{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrnn project\\nsequence to sequence translation task \\nusing custom tokenization and vectorization\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "rnn project\n",
    "sequence to sequence translation task \n",
    "using custom tokenization and vectorization\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -P \"../data\" http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "# !unzip ../data/spa-eng.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.read() method:\\nappropriate when you want to read the entire contents of a file into a string.\\n\\nIf the file is very large and you only need to process it line by line or in chunks, you might prefer to use \\n.readline() or iterate over the file object directly. This approach is more memory-efficient.\\n\\nas the data is not so big .read() is used in this project\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    ".read() method:\n",
    "appropriate when you want to read the entire contents of a file into a string.\n",
    "\n",
    "If the file is very large and you only need to process it line by line or in chunks, you might prefer to use \n",
    ".readline() or iterate over the file object directly. This approach is more memory-efficient.\n",
    "\n",
    "as the data is not so big .read() is used in this project\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVe.\n",
      "7\n",
      "118964\n"
     ]
    }
   ],
   "source": [
    "with open ('../data/spa-eng/spa.txt', 'r') as f:\n",
    "        lines = f.read().split('\\n')[:-1] # '\\n' : split the data line by line\n",
    "\n",
    "print(lines[0])\n",
    "print(len(lines[0]))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\n",
      "Ve.\n",
      "118964\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "source_data = []\n",
    "target_data = []\n",
    "\n",
    "for line in lines:\n",
    "    source, target = line.split('\\t') # '\\t' : split the data by the space\n",
    "    source_data.append(source)\n",
    "    target_data.append(target)\n",
    "    data.append((source, target))\n",
    "\n",
    "print(source_data[0])\n",
    "print(target_data[0])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "num_val_samples = int(len(data)*0.15)\n",
    "num_train_samples = len(data) - 2*num_val_samples\n",
    "\n",
    "train_pairs = data[:num_train_samples]\n",
    "val_pairs = data[num_train_samples: num_train_samples + num_val_samples]\n",
    "test_pairs = data[num_train_samples + num_val_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer():\n",
    "    def __init__(self, sequence_length, vocab_size, target = False):\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target = target\n",
    "        self.vocab_counter = Counter()\n",
    "        self.stoi = {'[pad]':0, '[start]':1, '[end]':2, '[unkown]':3}\n",
    "        self.itos = {0:'[pad]', 1:'[start]', 2:'[end]', 3:'[unkown]'}\n",
    "\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text\n",
    "                        if char not in strip_chars)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def adapt(self, dataset):\n",
    "        \n",
    "        for text in tqdm(dataset):\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                self.vocab_counter[token] += 1\n",
    "\n",
    "        for token, _ in self.vocab_counter.most_common(self.vocab_size):\n",
    "            index = len(self.stoi)\n",
    "            self.stoi[token] = index\n",
    "            self.itos[index] = token\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        if self.target:\n",
    "            result = ([self.stoi['[start]']]+ [self.stoi.get(token, 3) for token in tokens]\n",
    "                    + [self.stoi['[end]']])\n",
    "        else:\n",
    "            result = [self.stoi.get(token, 3) for token in tokens]\n",
    "        \n",
    "        if len(result) <= self.sequence_length:\n",
    "            pad_size = self.sequence_length - len(result)\n",
    "            result += [self.stoi.get('[pad]')] * (pad_size)\n",
    "        else:\n",
    "            #truncate!\n",
    "            result = result[:self.sequence_length]    \n",
    "        return result\n",
    "        \n",
    "    def decode(self, int_sequence):\n",
    "        \n",
    "        return \" \".join(self.itos.get(i , '[unknown]') for i in int_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "vocab_size = 15000\n",
    "\n",
    "source_vectorizer = TextVectorizer(sequence_length, vocab_size)\n",
    "target_vectorizer = TextVectorizer(sequence_length +1, vocab_size, target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118964/118964 [00:00<00:00, 277019.84it/s]\n",
      "100%|██████████| 118964/118964 [00:00<00:00, 267470.15it/s]\n"
     ]
    }
   ],
   "source": [
    "source_vectorizer.adapt(source_data)\n",
    "target_vectorizer.adapt(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if you want to sound [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ = source_vectorizer.encode('If you want to sound')\n",
    "source_vectorizer.decode(encoded_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What time do you get up every day?\n",
      "¿A qué hora te levantas todos los días?\n"
     ]
    }
   ],
   "source": [
    "eng, spa = data[700]\n",
    "print(eng)\n",
    "print(spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what time do you get up every day [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
      "[start] a qué hora te levantas todos los días [end] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n"
     ]
    }
   ],
   "source": [
    "print(source_vectorizer.decode(source_vectorizer.encode(eng)))\n",
    "print(target_vectorizer.decode(target_vectorizer.encode(spa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "print(len(source_vectorizer.decode(source_vectorizer.encode(eng))))\n",
    "print(len(target_vectorizer.decode(source_vectorizer.encode(eng))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    def __init__(self, data, source_vectorizer, target_vectorizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.source_vectorizer = source_vectorizer\n",
    "        self.target_vectorizer = target_vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        eng, spa = self.data[index]\n",
    "        eng = self.source_vectorizer.encode(eng)\n",
    "        spa = self.target_vectorizer.encode(spa)\n",
    "        return ({\n",
    "            'english': torch.tensor(eng).long(),\n",
    "            'spanish': torch.tensor(spa[:-1]).long()\n",
    "            }, torch.tensor(spa[1:]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EngSpaDataset(train_pairs, source_vectorizer, target_vectorizer)\n",
    "val_dataset = EngSpaDataset(val_pairs, source_vectorizer, target_vectorizer)\n",
    "test_dataset = EngSpaDataset(test_pairs, source_vectorizer, target_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]['spanish'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhy the collate_fn ->\\n\\n- permute() method : to change the order of dimensions of a tensor.\\n- handle variable length (however, it is handled by TextVectorizer already)\\n- much more organized data\\n\\n(also possible direct indexing without zero-initialized tensors)\\nzero-initialized tensors: \\n- prepare for padding\\n- control data storage\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "why the collate_fn ->\n",
    "\n",
    "- permute() method : to change the order of dimensions of a tensor.\n",
    "- handle variable length (however, it is handled by TextVectorizer already)\n",
    "- much more organized data\n",
    "\n",
    "(also possible direct indexing without zero-initialized tensors)\n",
    "zero-initialized tensors: \n",
    "- prepare for padding\n",
    "- control data storage\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_batch_seq_collate(data: torch.Tensor): # data-> is a batch of the data\n",
    "  batch_size = len(data)  \n",
    "  source_input = torch.zeros(batch_size, data[0][0][\"english\"].size(0))\n",
    "  target_input = torch.zeros(batch_size, data[0][0][\"spanish\"].size(0))\n",
    "  target_output = torch.zeros(batch_size, data[0][1].size(0))\n",
    "  for idx, (inputs, output) in enumerate(data):\n",
    "    source_input[idx] = inputs[\"english\"]\n",
    "    target_input[idx] = inputs[\"spanish\"]\n",
    "    target_output[idx] = output\n",
    "\n",
    "  return (source_input.permute(1, 0).long(), target_input.permute(1, 0).long(),\n",
    "          target_output.permute(1, 0).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn= permute_batch_seq_collate)\n",
    "val_dl = DataLoader(val_dataset, batch_size, collate_fn= permute_batch_seq_collate)\n",
    "test_dl = DataLoader(test_dataset, batch_size, collate_fn= permute_batch_seq_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n",
      "torch.Size([20, 64])\n",
      "torch.Size([20, 64])\n"
     ]
    }
   ],
   "source": [
    "source_, target_input_ , target_output_ = next(iter(train_dl))\n",
    "print(source_.size())\n",
    "print(target_input_.size())\n",
    "print(target_output_.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, size = (20, 64)) # torch.Size([20, 64]) integers between 0 to vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_dim: int, embedding_dim: int, hidden_dim: int, \n",
    "                 padding_index:int =0, num_rnn_layers: int= 1, dropout= 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding_layer = nn.Embedding(source_dim, embedding_dim, \n",
    "                                            padding_idx=padding_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                                  num_layers= num_rnn_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        output, (cell_state, hidden_state) = self.lstm_layer(x)\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "        # output size: [seq_len, batch_size, hidden_dim] which we don't need in this model\n",
    "        # cell_state , hidden_state: [1, batch_size, hidden_dim] \n",
    "        # cell_ state and hidde_state passed to Decoder(input cell and input hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512])\n",
      "torch.Size([1, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, len(source_vectorizer.stoi), size = (20, 64))\n",
    "encoder_ = Encoder(len(source_vectorizer.stoi), 256, 512)\n",
    "print(encoder_(x)[0].size())\n",
    "print(encoder_(x)[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_dim:int, embedding_dim: int, hidden_dim:int, \n",
    "                 padding_index: int= 0, num_rnn_layers:int =1,  dropout= 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(target_dim, embedding_dim,  \n",
    "                                            padding_idx=padding_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                                  num_layers=num_rnn_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, target_dim)\n",
    "\n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        outputs , (cell, hidden) = self.lstm_layer(x, (hidden_state, cell_state)) \n",
    "        predictions = self.classifier(outputs)      # outputs: [seq_len, batch_size, hidden_dim]\n",
    "        return predictions                          # predictions: [seq_len, batch_size, target_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64, 15004])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randint(0, len(target_vectorizer.stoi), size=(20,64))\n",
    "decoder_ = Decoder(len(target_vectorizer.stoi), 256, 512)\n",
    "decoder_(y, encoder_(x)[0], encoder_(x)[1]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        h , c = self.encoder(source)\n",
    "        outputs = self.decoder(target, h, c)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64, 15004])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ = LSTMNet(encoder_, decoder_)\n",
    "model_(x, y).size()\n",
    "# len(target_vectorizer.stoi) = 15004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anazarnia/data_science/myenv_1/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "source_vocab_size = len(source_vectorizer.stoi) # 13636\n",
    "target_vocab_size = len(target_vectorizer.stoi) # 15004\n",
    "encoder_embedding = 128\n",
    "decoder_embedding = 128\n",
    "hidden_dim = 256\n",
    "padding_index = target_vectorizer.stoi['[pad]'] # 0\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "\n",
    "encoder = Encoder(\n",
    "    source_vocab_size,\n",
    "    encoder_embedding,\n",
    "    hidden_dim\n",
    "    ).to(device)\n",
    "decoder = Decoder(\n",
    "    target_vocab_size,\n",
    "    decoder_embedding,\n",
    "    hidden_dim\n",
    "    ).to(device)\n",
    "\n",
    "model = LSTMNet(encoder, decoder)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index= padding_index)\n",
    "optimizer = torch.optim.NAdam(model.parameters(), lr= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\neach token from target_out (true class index) compared vs the corresponding row in predictions.\\nthe row in predictions gives a probability distribution across all classes (target_dim)\\n[(seq_len * batch_size), target_dim] vs [seq_len * batch_size]\\n\\nEach row in predictions corresponds to one token in the sequence and contains target_dim values, \\neach representing the model's predicted score (or probability if softmaxed) for each possible class\\n\\nInternally, CrossEntropyLoss applies the softmax function to convert scores in predictions to probabilities \\nand then uses the negative log likelihood of the probability assigned to the correct class as the loss.\\n\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "each token from target_out (true class index) compared vs the corresponding row in predictions.\n",
    "the row in predictions gives a probability distribution across all classes (target_dim)\n",
    "[(seq_len * batch_size), target_dim] vs [seq_len * batch_size]\n",
    "\n",
    "Each row in predictions corresponds to one token in the sequence and contains target_dim values, \n",
    "each representing the model's predicted score (or probability if softmaxed) for each possible class\n",
    "\n",
    "Internally, CrossEntropyLoss applies the softmax function to convert scores in predictions to probabilities \n",
    "and then uses the negative log likelihood of the probability assigned to the correct class as the loss.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def accuracy(predictions, true_target):\n",
    "#     predicted_tokens = predictions.view(-1, predictions.shape[-1]).argmax(dim=1)\n",
    "#     true_target_flat = true_target.reshape(-1)\n",
    "#     correct_tokens = (predicted_tokens == true_target_flat).sum().item()\n",
    "#     return correct_tokens / true_target_flat.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, true_tokens):  # [seq_len , batch_size, vocab_size] , [seq_len , batch_size]\n",
    "    predicted_tokens = predictions.view(-1, predictions.shape[-1]).argmax(dim=1) # [seq_len*batch_size] torch.Size([1280])\n",
    "    true_tokens_flat = true_tokens.reshape(-1)                      # [seq_len*batch_size] torch.Size([1280])\n",
    "    correct = 0 \n",
    "    for idx, token in enumerate(predicted_tokens):  # len(predicted_tokens) = 1280\n",
    "\n",
    "        if token == true_tokens_flat[idx]:\n",
    "            correct += 1\n",
    "        if token == 0:\n",
    "            break\n",
    "    return correct / (idx+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25831)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ = torch.randint(0, 15004, size= (20, 64, 15004))\n",
    "x_.view(-1, x_.shape[-1]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for source, target_in, target_out in tqdm(train_dl):\n",
    "        \n",
    "        source = source.to(device)\n",
    "        target_in = target_in.to(device)\n",
    "        target_out = target_out.to(device)                 # [seq_len, batch_size]           \n",
    "        predictions = model(source, target_in)             # [seq_len, batch_size, target_dim]\n",
    "        # target_out.reshape(-1) -> [seq_len * batch_size]\n",
    "        # predictions.view(-1, predictions.shape[-1]) -> [(seq_len * batch_size), target_dim] \n",
    "        loss = criterion(predictions.reshape(-1, predictions.shape[-1]), target_out.reshape(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        with torch.no_grad():\n",
    "            train_acc += accuracy(predictions, target_out)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for source, target_in, target_out in val_dl:\n",
    "            source = source.to(device)\n",
    "            target_in = target_in.to(device)\n",
    "            target_out = target_out.to(device)           \n",
    "            predictions = model(source, target_in)\n",
    "\n",
    "            loss = criterion(predictions.reshape(-1, predictions.shape[-1]), target_out.reshape(-1))\n",
    "            val_loss += loss\n",
    "            val_acc += accuracy(predictions, target_out)\n",
    "\n",
    "    print (f'Epoch {epoch+1}/{num_epochs} |\\\n",
    "            Train Loss {train_loss:.2f}, Train Accuracy {(train_acc / len(train_dl)):.2f} |\\\n",
    "            Validation Loss {val_loss:.2f}, Validation Accuracy {(val_acc / len(val_dl)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 27, 272, 102,  39,  51,  13, 167,   2,   2,   2,   2,   2,   2,   2,\n",
      "          2,   2,   2,   2,   2,   2], device='mps:0')\n",
      "tensor([  27, 2821,    7,  177,  329,   13, 7074,    2,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "source, target_in, target_out = next(iter(val_dl))\n",
    "predicted_tokens = model(source.to(device), target_in.to(device))\n",
    "predicted_vector = predicted_tokens[:, 18].argmax(1) # predicted vector for the 18th batch\n",
    "target_vector  = target_out[:, 18] # original target vector for the 18th batch\n",
    "print(predicted_vector)\n",
    "print(target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' qué pasó de tiene haber un auto [end]'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = ''\n",
    "for i in predicted_vector:\n",
    "    text = target_vectorizer.itos[i.item()]\n",
    "    translate += ' '+text\n",
    "    if text == '[end]':\n",
    "        break\n",
    "translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' qué alimentos no debería tomar un diabético [end]'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate = ''\n",
    "for i in target_vector:\n",
    "    text = target_vectorizer.itos[i.item()]\n",
    "    translate += ' '+text\n",
    "    if text == '[end]':\n",
    "        break\n",
    "translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
