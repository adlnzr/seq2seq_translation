{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrnn project\\nsequence to sequence translation task \\nusing custom tokenization and vectorization\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "rnn project\n",
    "sequence to sequence translation task \n",
    "using custom tokenization and vectorization\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -P \"../data\" http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "# !unzip ../data/spa-eng.zip -d ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.read() method:\\nappropriate when you want to read the entire contents of a file into a string.\\n\\nIf the file is very large and you only need to process it line by line or in chunks, you might prefer to use \\n.readline() or iterate over the file object directly. This approach is more memory-efficient.\\n\\nas the data is not so big .read() is used in this project\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    ".read() method:\n",
    "appropriate when you want to read the entire contents of a file into a string.\n",
    "\n",
    "If the file is very large and you only need to process it line by line or in chunks, you might prefer to use \n",
    ".readline() or iterate over the file object directly. This approach is more memory-efficient.\n",
    "\n",
    "as the data is not so big .read() is used in this project\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVe.\n",
      "7\n",
      "118964\n"
     ]
    }
   ],
   "source": [
    "with open ('../data/spa-eng/spa.txt', 'r') as f:\n",
    "        lines = f.read().split('\\n')[:-1] # '\\n' : split the data line by line\n",
    "\n",
    "print(lines[0])\n",
    "print(len(lines[0]))\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\n",
      "Ve.\n",
      "118964\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "source_data = []\n",
    "target_data = []\n",
    "\n",
    "for line in lines:\n",
    "    source, target = line.split('\\t') # '\\t' : split the data by the space\n",
    "    source_data.append(source)\n",
    "    target_data.append(target)\n",
    "    data.append((source, target))\n",
    "\n",
    "print(source_data[0])\n",
    "print(target_data[0])\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "num_val_samples = int(len(data)*0.15)\n",
    "num_train_samples = len(data) - 2*num_val_samples\n",
    "\n",
    "train_pairs = data[:num_train_samples]\n",
    "val_pairs = data[num_train_samples: num_train_samples + num_val_samples]\n",
    "test_pairs = data[num_train_samples + num_val_samples: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorizer():\n",
    "    def __init__(self, sequence_length, vocab_size, target = False):\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.target = target\n",
    "        self.vocab_counter = Counter()\n",
    "        self.stoi = {'[pad]':0, '[start]':1, '[end]':2, '[unkown]':3}\n",
    "        self.itos = {0:'[pad]', 1:'[start]', 2:'[end]', 3:'[unkown]'}\n",
    "\n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return \"\".join(char for char in text\n",
    "                        if char not in strip_chars)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def adapt(self, dataset):\n",
    "        \n",
    "        for text in tqdm(dataset):\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                self.vocab_counter[token] += 1\n",
    "\n",
    "        for token, _ in self.vocab_counter.most_common(self.vocab_size):\n",
    "            index = len(self.stoi)\n",
    "            self.stoi[token] = index\n",
    "            self.itos[index] = token\n",
    "\n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "\n",
    "        if self.target:\n",
    "            result = ([self.stoi['[start]']]+ [self.stoi.get(token, 3) for token in tokens]\n",
    "                    + [self.stoi['[end]']])\n",
    "        else:\n",
    "            result = [self.stoi.get(token, 3) for token in tokens]\n",
    "        \n",
    "        if len(result) <= self.sequence_length:\n",
    "            pad_size = self.sequence_length - len(result)\n",
    "            result += [self.stoi.get('[pad]')] * (pad_size)\n",
    "        else:\n",
    "            #truncate!\n",
    "            result = result[:self.sequence_length]    \n",
    "        return result\n",
    "        \n",
    "    def decode(self, int_sequence):\n",
    "        \n",
    "        return \" \".join(self.itos.get(i , '[unknown]') for i in int_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "vocab_size = 15000\n",
    "\n",
    "source_vectorizer = TextVectorizer(sequence_length, vocab_size)\n",
    "target_vectorizer = TextVectorizer(sequence_length +1, vocab_size, target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118964/118964 [00:00<00:00, 289830.25it/s]\n",
      "100%|██████████| 118964/118964 [00:00<00:00, 268507.75it/s]\n"
     ]
    }
   ],
   "source": [
    "source_vectorizer.adapt(source_data)\n",
    "target_vectorizer.adapt(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if you want to sound [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ = source_vectorizer.encode('If you want to sound')\n",
    "source_vectorizer.decode(encoded_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would I do without you?\n",
      "¿Qué haría yo sin vosotros?\n"
     ]
    }
   ],
   "source": [
    "eng, spa = data[700]\n",
    "print(eng)\n",
    "print(spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what would i do without you [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
      "[start] qué haría yo sin vosotros [end] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n"
     ]
    }
   ],
   "source": [
    "print(source_vectorizer.decode(source_vectorizer.encode(eng)))\n",
    "print(target_vectorizer.decode(target_vectorizer.encode(spa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "print(len(source_vectorizer.decode(source_vectorizer.encode(eng))))\n",
    "print(len(target_vectorizer.decode(source_vectorizer.encode(eng))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    def __init__(self, data, source_vectorizer, target_vectorizer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = data\n",
    "        self.source_vectorizer = source_vectorizer\n",
    "        self.target_vectorizer = target_vectorizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        eng, spa = self.data[index]\n",
    "        eng = self.source_vectorizer.encode(eng)\n",
    "        spa = self.target_vectorizer.encode(spa)\n",
    "        return ({\n",
    "            'english': torch.tensor(eng).long(),\n",
    "            'spanish': torch.tensor(spa[:-1]).long()\n",
    "            }, torch.tensor(spa[1:]).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EngSpaDataset(train_pairs, source_vectorizer, target_vectorizer)\n",
    "val_dataset = EngSpaDataset(val_pairs, source_vectorizer, target_vectorizer)\n",
    "test_dataset = EngSpaDataset(test_pairs, source_vectorizer, target_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0][0]['spanish'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhy the collate_fn ->\\n\\n- permute() method : to change the order of dimensions of a tensor.\\n- handle variable length (however, it is handled by TextVectorizer already)\\n- much more organized data\\n\\n(also possible direct indexing without zero-initialized tensors)\\nzero-initialized tensors: \\n- prepare for padding\\n- control data storage\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "why the collate_fn ->\n",
    "\n",
    "- permute() method : to change the order of dimensions of a tensor.\n",
    "- handle variable length (however, it is handled by TextVectorizer already)\n",
    "- much more organized data\n",
    "\n",
    "(also possible direct indexing without zero-initialized tensors)\n",
    "zero-initialized tensors: \n",
    "- prepare for padding\n",
    "- control data storage\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_batch_seq_collate(data: torch.Tensor): # data-> is a batch of the data\n",
    "  batch_size = len(data)  \n",
    "  source_input = torch.zeros(batch_size, data[0][0][\"english\"].size(0))\n",
    "  target_input = torch.zeros(batch_size, data[0][0][\"spanish\"].size(0))\n",
    "  target_output = torch.zeros(batch_size, data[0][1].size(0))\n",
    "  for idx, (inputs, output) in enumerate(data):\n",
    "    source_input[idx] = inputs[\"english\"]\n",
    "    target_input[idx] = inputs[\"spanish\"]\n",
    "    target_output[idx] = output\n",
    "\n",
    "  return (source_input.permute(1, 0).long(), target_input.permute(1, 0).long(),\n",
    "          target_output.permute(1, 0).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_ds = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn= permute_batch_seq_collate)\n",
    "val_ds = DataLoader(val_dataset, batch_size, collate_fn= permute_batch_seq_collate)\n",
    "test_ds = DataLoader(test_dataset, batch_size, collate_fn= permute_batch_seq_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 64])\n",
      "torch.Size([20, 64])\n",
      "torch.Size([20, 64])\n"
     ]
    }
   ],
   "source": [
    "source_, target_input_ , target_output_ = next(iter(train_ds))\n",
    "print(source_.size())\n",
    "print(target_input_.size())\n",
    "print(target_output_.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, vocab_size, size = (20, 64)) # torch.Size([20, 64]) integers between 0 to vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_dim: int, embedding_dim: int, hidden_dim: int, \n",
    "                 padding_index:int =0, num_rnn_layers: int= 1, dropout= 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embedding_layer = nn.Embedding(source_dim, embedding_dim, \n",
    "                                            padding_idx=padding_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                                  num_layers= num_rnn_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        output, (cell_state, hidden_state) = self.lstm_layer(x)\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "        # output size: [seq_len, batch_size, hidden_dim] which we don't need in this model\n",
    "        # cell_state , hidden_state: [1, batch_size, hidden_dim] \n",
    "        # cell_ state and hidde_state passed to Decoder(input cell and input hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 512])\n",
      "torch.Size([1, 64, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, len(source_vectorizer.stoi), size = (20, 64))\n",
    "encoder_ = Encoder(len(source_vectorizer.stoi), 256, 512)\n",
    "print(encoder_(x)[0].size())\n",
    "print(encoder_(x)[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13636"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_dim:int, embedding_dim: int, hidden_dim:int, \n",
    "                 padding_index: int= 0, num_rnn_layers:int =1,  dropout = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_layer = nn.Embedding(target_dim, embedding_dim, \n",
    "                                            padding_index=padding_index)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, hidden_dim, \n",
    "                                  num_layers=n_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, cell_state, hidden_state):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.dropout(x)\n",
    "        output , (cell, hidden) = self.lstm_layer(x, cell_state, hidden_state)\n",
    "        x = self.linear(output)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
